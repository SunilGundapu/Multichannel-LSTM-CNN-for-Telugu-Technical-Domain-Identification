{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, sys, string, io\n",
    "import pandas as pd\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import numpy as np\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras.preprocessing import text, sequence\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape:  (68865, 2)\n",
      "Validation Data Shape:  (5920, 2)\n",
      "Test Data Shape:  (2611, 1)\n"
     ]
    }
   ],
   "source": [
    "def getData(file):\n",
    "    tsv_file = open(file)\n",
    "    df = pd.read_csv(tsv_file, delimiter=\"\\t\")\n",
    "    return df\n",
    "\n",
    "trainFilename = \"sub-task-1h/sub-task-1h-train-te.tsv\"\n",
    "validFilename = \"sub-task-1h/sub-task-1h-dev-te.tsv\"\n",
    "testFilename = \"sub-task-1h/sub-task-1h-test-te.txt\"\n",
    "trainDF = getData(trainFilename)\n",
    "validDF = getData(validFilename)\n",
    "testDF = getData(testFilename)\n",
    "print(\"Train Data Shape: \",trainDF.shape)\n",
    "print(\"Validation Data Shape: \",validDF.shape)\n",
    "print(\"Test Data Shape: \",testDF.shape)\n",
    "\n",
    "#totalTextData = pd.concat([trainDF['text'], validDF['text'], testDF['text']])\n",
    "totalTextData = pd.concat([trainDF['text'], validDF['text']])\n",
    "#totalTextData = pd.concat([trainDF, validDF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cse         2175\n",
       "phy         1650\n",
       "com_tech     970\n",
       "bioche       580\n",
       "other        390\n",
       "mgmt         155\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validDF.label.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape:  (2611, 4)\n"
     ]
    }
   ],
   "source": [
    "svmResultFile = \"run_train_valid_test.tsv\"\n",
    "svmDF = getData(svmResultFile)\n",
    "print(\"Train Data Shape: \",svmDF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSVMDF.shape\n",
    "newSVMDF.to_csv('TeluguTechDOM_sub-task-1h-test-te_run-2.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocess(data):\n",
    "    temp = []\n",
    "    punctuations = list(string.punctuation)\n",
    "    digits = list(string.digits)\n",
    "    letters = list(string.ascii_letters)\n",
    "    #removePunctuations = str.maketrans('', '', string.punctuation)\n",
    "    #removeDigits = str.maketrans('', '', string.digits)\n",
    "    #removeAlphabets = str.maketrans('', '', string.ascii_letters)\n",
    "    for item in data:\n",
    "        item = item.split(\" \")\n",
    "        temp1 = []\n",
    "        for token in item:\n",
    "            if(len(token)>1 and token not in digits and token not in punctuations):\n",
    "                temp1.append(token)\n",
    "        temp1 = ' '.join(temp1)\n",
    "        #print(temp1)\n",
    "        #token = token.translate(removePunctuations)\n",
    "        #token = token.translate(removeDigits)\n",
    "        #item = item.translate(removeAlphabets)\n",
    "        #print(item.split)\n",
    "        temp.append(temp1)\n",
    "    return temp\n",
    "#print(string.digits)\n",
    "trainDF['processedText'] = dataPreprocess(trainDF['text'])\n",
    "validDF['processedText'] = dataPreprocess(validDF['text'])\n",
    "testDF['processedText'] = dataPreprocess(testDF['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedText = pd.concat([trainDF['processedText'], validDF['processedText'], testDF['processedText']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(totalTextData)\n",
    "xtrain_tfidf =  tfidf_vect.transform(trainDF['text'])\n",
    "xvalid_tfidf =  tfidf_vect.transform(validDF['text'])\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(totalTextData)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(trainDF['text'])\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(validDF['text'])\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=15000)\n",
    "tfidf_vect_ngram_chars.fit(totalTextData)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(trainDF['text']) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(validDF['text']) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(testDF['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149622,) (149622, 15000)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ROS = RandomOverSampler(random_state=11)\n",
    "ros_tfidf_text, ros_tfidf_labels = ROS.fit_sample(xtrain_tfidf_ngram_chars, trainDF['label'])\n",
    "print(ros_tfidf_labels.shape, ros_tfidf_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68865, 6)\n",
      "(5920, 6)\n"
     ]
    }
   ],
   "source": [
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "labelEncoder.fit(['phy','cse','other','com_tech','bioche','mgmt'])\n",
    "trainDF['numericalLabels'] = labelEncoder.transform(trainDF['label'])\n",
    "validDF['numericalLabels'] = labelEncoder.transform(validDF['label'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "trainLabels = encoder.fit_transform(trainDF['label'])\n",
    "trainLabels = [to_categorical(i, num_classes=6) for i in trainLabels]\n",
    "trainLabels = np.asarray(trainLabels)\n",
    "print(trainLabels.shape)\n",
    "validLabels = encoder.fit_transform(validDF['label'])\n",
    "validLabels = [to_categorical(i, num_classes=6) for i in validLabels]\n",
    "validLabels = np.asarray(validLabels)\n",
    "print(validLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of texts:  112.42214347797018\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for item in totalTextData:\n",
    "    lengths.append(len(item))\n",
    "    #print(len(item))\n",
    "print(\"Average length of texts: \", sum(lengths)/len(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('TeluguFasttextVectors/cc.te.300.vec')):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(totalTextData)\n",
    "word_index = token.word_index\n",
    "\n",
    "input_size=150\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(trainDF['text']), maxlen=input_size)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(validDF['text']), maxlen=input_size)\n",
    "\n",
    "\n",
    "'''#create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#Save to pickle        \n",
    "with open('wordEmbeddingsMatrix.pickle', 'wb') as handle:\n",
    "    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)'''\n",
    "    \n",
    "#Load pickle\n",
    "with open('wordEmbeddingsMatrix.pickle', 'rb') as handle:\n",
    "    embedding_matrix = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, feature_vector_test, epoch=False, is_neural_net=False):\n",
    "    if is_neural_net:\n",
    "        f = open(\"score.txt\", \"w\")\n",
    "        classifier.fit(feature_vector_train, label, epochs=epoch)\n",
    "    \n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "        predictions1 = predictions.argmax(axis=-1)\n",
    "        validLabels1 = validLabels.argmax(axis=-1)\n",
    "        #print(predictions, predictions1)\n",
    "        #print(validLabels, validLabels1)\n",
    "        acc = metrics.accuracy_score(predictions1, validLabels1)\n",
    "        f1Score = metrics.f1_score(predictions1, validLabels1, average='macro')\n",
    "        classificationReport = classification_report(predictions1, validLabels1)\n",
    "        f.write(str(acc)+\"\\n\")\n",
    "        f.write(str(f1Score)+\"\\n\")\n",
    "        #f.write(classifier.history)\n",
    "        \n",
    "    else:\n",
    "        # fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "        # predict the labels on validation dataset\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "        validDF['predicted_label']=predictions\n",
    "        test_predictions = classifier.predict(feature_vector_test)\n",
    "        testDF['predicted_label'] = test_predictions\n",
    "        acc = metrics.accuracy_score(predictions, validDF['label'])\n",
    "        f1Score = metrics.f1_score(predictions, validDF['label'], average='macro')\n",
    "        classificationReport = classification_report(predictions, validDF['label'])\n",
    "    \n",
    "    return acc, f1Score, classificationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy, f1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors:  0.8827702702702702 0.8702677684005882 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      bioche       0.89      0.96      0.93       538\n",
      "    com_tech       0.91      0.91      0.91       969\n",
      "         cse       0.92      0.83      0.87      2391\n",
      "        mgmt       0.74      0.96      0.83       119\n",
      "       other       0.74      0.84      0.79       340\n",
      "         phy       0.86      0.91      0.89      1563\n",
      "\n",
      "    accuracy                           0.88      5920\n",
      "   macro avg       0.84      0.90      0.87      5920\n",
      "weighted avg       0.89      0.88      0.88      5920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1_Score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, trainDF['label'], xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy, f1_Score)\n",
    "\n",
    "#Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1_Score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, trainDF['label'], xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors: \", accuracy, f1_Score)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score, classReport = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars, xtest_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors: \", accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ఒకవేళ సమూహ సభ్యులు చర్చను పక్కదారి మళ్లిస్తున్...</td>\n",
       "      <td>com_tech</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ఈ ఆంటీబాడీలు మరియు అసైటిస్ ద్రవంలో ఉత్పత్తి చే...</td>\n",
       "      <td>bioche</td>\n",
       "      <td>bioche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>కాబట్టి , ఈ కవచం దరఖాస్తు చేసుకుంటే , ద్రవ కణం...</td>\n",
       "      <td>phy</td>\n",
       "      <td>phy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ఇది ఒక నమూనా మాత్రమే .</td>\n",
       "      <td>com_tech</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>మనము రిసోర్సస్  నుండి తీసిన మాదిరిగానే ఒక సరళమ...</td>\n",
       "      <td>cse</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915</th>\n",
       "      <td>VNet ఇది ఒక వర్చువల్ ప్రైవేట్ నెట్వర్క్  మరియు...</td>\n",
       "      <td>cse</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5916</th>\n",
       "      <td>డెల్టా ఆంగిల్ మారదు కాని ఇది షుగర్ ఆకృతీకరణపై ...</td>\n",
       "      <td>bioche</td>\n",
       "      <td>bioche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>ఇది విలక్షణమైన పట్టణము వంటి ఒక శిథిలమైన ఎక్స్ప...</td>\n",
       "      <td>cse</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5918</th>\n",
       "      <td>ఎందుకు , మీరు నష్టాలను అధిగమించడానికి ఎందుకంటే .</td>\n",
       "      <td>phy</td>\n",
       "      <td>cse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5919</th>\n",
       "      <td>మన తప్పుల నుంచి మనం అనేక పాఠాలు నేర్చుకొని ముం...</td>\n",
       "      <td>com_tech</td>\n",
       "      <td>com_tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5920 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label  \\\n",
       "0     ఒకవేళ సమూహ సభ్యులు చర్చను పక్కదారి మళ్లిస్తున్...  com_tech   \n",
       "1     ఈ ఆంటీబాడీలు మరియు అసైటిస్ ద్రవంలో ఉత్పత్తి చే...    bioche   \n",
       "2     కాబట్టి , ఈ కవచం దరఖాస్తు చేసుకుంటే , ద్రవ కణం...       phy   \n",
       "3                                ఇది ఒక నమూనా మాత్రమే .  com_tech   \n",
       "4     మనము రిసోర్సస్  నుండి తీసిన మాదిరిగానే ఒక సరళమ...       cse   \n",
       "...                                                 ...       ...   \n",
       "5915  VNet ఇది ఒక వర్చువల్ ప్రైవేట్ నెట్వర్క్  మరియు...       cse   \n",
       "5916  డెల్టా ఆంగిల్ మారదు కాని ఇది షుగర్ ఆకృతీకరణపై ...    bioche   \n",
       "5917  ఇది విలక్షణమైన పట్టణము వంటి ఒక శిథిలమైన ఎక్స్ప...       cse   \n",
       "5918   ఎందుకు , మీరు నష్టాలను అధిగమించడానికి ఎందుకంటే .       phy   \n",
       "5919  మన తప్పుల నుంచి మనం అనేక పాఠాలు నేర్చుకొని ముం...  com_tech   \n",
       "\n",
       "     predicted_label  \n",
       "0           com_tech  \n",
       "1             bioche  \n",
       "2                phy  \n",
       "3                cse  \n",
       "4                cse  \n",
       "...              ...  \n",
       "5915             cse  \n",
       "5916          bioche  \n",
       "5917             cse  \n",
       "5918             cse  \n",
       "5919        com_tech  \n",
       "\n",
       "[5920 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OvsRestLR, CharLevel Vectors:  0.8760135135135135 0.8647051021742347 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      bioche       0.90      0.97      0.93       540\n",
      "    com_tech       0.91      0.90      0.91       976\n",
      "         cse       0.91      0.83      0.87      2404\n",
      "        mgmt       0.72      0.97      0.83       116\n",
      "       other       0.71      0.87      0.78       315\n",
      "         phy       0.85      0.90      0.87      1569\n",
      "\n",
      "    accuracy                           0.88      5920\n",
      "   macro avg       0.83      0.91      0.86      5920\n",
      "weighted avg       0.88      0.88      0.88      5920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = OneVsRestClassifier(linear_model.LogisticRegression())\n",
    "accuracy, f1_Score, classReport = train_model(model, xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars, xtest_tfidf_ngram_chars)\n",
    "print(\"OvsRestLR, CharLevel Vectors: \", accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, trainDF['label'], xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, WordLevel TF-IDF:  0.668918918918919 0.5761793854482674\n"
     ]
    }
   ],
   "source": [
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy, f1_Score = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, trainDF['label'], xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy, f1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "#accuracy, f1_Score = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), trainDF['label'], xvalid_tfidf.tocsc())\n",
    "#print(\"Xgb, WordLevel TF-IDF: \", accuracy, f1_Score)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy, f1_Score = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), trainDF['label'], xvalid_tfidf_ngram_chars.tocsc(), xtest_tfidf_ngram_chars.tocsc())\n",
    "print(\"Xgb, CharLevel Vectors: \", accuracy, f1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.37803929\n",
      "Iteration 2, loss = 0.77890729\n",
      "Iteration 3, loss = 0.53531725\n",
      "Iteration 4, loss = 0.42971023\n",
      "Iteration 5, loss = 0.37098173\n",
      "Iteration 6, loss = 0.33247569\n",
      "Iteration 7, loss = 0.30437601\n",
      "Iteration 8, loss = 0.28259671\n",
      "Iteration 9, loss = 0.26479156\n",
      "Iteration 10, loss = 0.24994096\n",
      "Iteration 11, loss = 0.23697560\n",
      "Iteration 12, loss = 0.22569611\n",
      "Iteration 13, loss = 0.21564287\n",
      "Iteration 14, loss = 0.20655091\n",
      "Iteration 15, loss = 0.19841531\n",
      "Iteration 16, loss = 0.19093693\n",
      "Iteration 17, loss = 0.18393985\n",
      "Iteration 18, loss = 0.17766783\n",
      "Iteration 19, loss = 0.17169291\n",
      "Iteration 20, loss = 0.16617635\n",
      "Iteration 21, loss = 0.16099913\n",
      "Iteration 22, loss = 0.15623947\n",
      "MLP, WordLevel TF-IDF:  0.9003378378378378 0.8936647524524427 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      bioche       0.94      0.96      0.95       567\n",
      "    com_tech       0.93      0.94      0.93       961\n",
      "         cse       0.91      0.87      0.89      2259\n",
      "        mgmt       0.83      0.93      0.88       138\n",
      "       other       0.79      0.83      0.81       372\n",
      "         phy       0.90      0.91      0.90      1623\n",
      "\n",
      "    accuracy                           0.90      5920\n",
      "   macro avg       0.88      0.91      0.89      5920\n",
      "weighted avg       0.90      0.90      0.90      5920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(22,23):\n",
    "    # Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "    accuracy, f1_Score, classReport = train_model(MLPClassifier(hidden_layer_sizes= (256,), learning_rate_init=0.0001, activation='relu', n_iter_no_change=True, verbose=True, random_state=1, max_iter=i), xtrain_tfidf_ngram_chars, trainDF['label'], xvalid_tfidf_ngram_chars, xtest_tfidf_ngram_chars)\n",
    "    print(\"MLP, WordLevel TF-IDF: \",  accuracy, f1_Score,\"\\n\", classReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bioche      863\n",
       "cse         594\n",
       "phy         355\n",
       "com_tech    297\n",
       "other       269\n",
       "mgmt        233\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.predicted_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bioche      871\n",
       "cse         579\n",
       "phy         352\n",
       "com_tech    300\n",
       "other       269\n",
       "mgmt        240\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.predicted_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.to_csv('TeluguTechDOM_sub-task-1h-test-te_run-3.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bioche      681\n",
       "cse         471\n",
       "phy         286\n",
       "com_tech    242\n",
       "other       217\n",
       "mgmt        201\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.predicted_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bioche      668\n",
       "cse         501\n",
       "phy         289\n",
       "com_tech    246\n",
       "other       212\n",
       "mgmt        182\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.predicted_label.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2153/2153 [==============================] - 510s 237ms/step - loss: 0.7274\n",
      "RNN-LSTM, Word Embeddings (0.7826013513513513, 0.7705641380399767)\n"
     ]
    }
   ],
   "source": [
    "def create_rnn_lstm(input_size):    \n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    #embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(150)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(150, activation=\"tanh\")(lstm_layer)\n",
    "    #output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(6, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm(input_size)\n",
    "accuracy = train_model(classifier, train_seq_x, trainLabels, valid_seq_x,1, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_rnn_bilstm_attention(input_size):    \n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    #embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(300, return_sequences=True, dropout=0.25,recurrent_dropout=0.25))(embedding_layer)\n",
    "    \n",
    "    #Self Attention\n",
    "    attention_layer = Attention(input_size)(lstm_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(256, activation=\"relu\")(attention_layer)\n",
    "    #output_layer1 = layers.Dropout(0.2)(output_layer1)\n",
    "    output_layer2 = layers.Dense(6, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_bilstm_attention(input_size)\n",
    "accuracy = train_model(classifier, train_seq_x, trainLabels, valid_seq_x,5, is_neural_net=True)\n",
    "print(\"RNN-LSTM, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2153/2153 [==============================] - 144s 67ms/step - loss: 0.6870\n",
      "Epoch 2/25\n",
      "2153/2153 [==============================] - 161s 75ms/step - loss: 0.4975\n",
      "Epoch 3/25\n",
      "2153/2153 [==============================] - 163s 76ms/step - loss: 0.4265\n",
      "Epoch 4/25\n",
      "2153/2153 [==============================] - 159s 74ms/step - loss: 0.3793\n",
      "Epoch 5/25\n",
      "2153/2153 [==============================] - 159s 74ms/step - loss: 0.3439\n",
      "Epoch 6/25\n",
      "2153/2153 [==============================] - 169s 79ms/step - loss: 0.3187\n",
      "Epoch 7/25\n",
      "2153/2153 [==============================] - 157s 73ms/step - loss: 0.2926\n",
      "Epoch 8/25\n",
      "2153/2153 [==============================] - 156s 73ms/step - loss: 0.2775\n",
      "Epoch 9/25\n",
      "2153/2153 [==============================] - 157s 73ms/step - loss: 0.2683\n",
      "Epoch 10/25\n",
      "2153/2153 [==============================] - 157s 73ms/step - loss: 0.2502\n",
      "Epoch 11/25\n",
      "2153/2153 [==============================] - 33530s 16s/step - loss: 0.2365\n",
      "Epoch 12/25\n",
      "2153/2153 [==============================] - 138s 64ms/step - loss: 0.2261\n",
      "Epoch 13/25\n",
      "2153/2153 [==============================] - 140s 65ms/step - loss: 0.2163\n",
      "Epoch 14/25\n",
      "2153/2153 [==============================] - 139s 65ms/step - loss: 0.2119\n",
      "Epoch 15/25\n",
      "2153/2153 [==============================] - 140s 65ms/step - loss: 0.2013\n",
      "Epoch 16/25\n",
      "2153/2153 [==============================] - 140s 65ms/step - loss: 0.1951\n",
      "Epoch 17/25\n",
      "2153/2153 [==============================] - 142s 66ms/step - loss: 0.1917\n",
      "Epoch 18/25\n",
      "2153/2153 [==============================] - 141s 65ms/step - loss: 0.1829\n",
      "Epoch 19/25\n",
      "2153/2153 [==============================] - 9548s 4s/step - loss: 0.1812\n",
      "Epoch 20/25\n",
      "2153/2153 [==============================] - 123s 57ms/step - loss: 0.1788\n",
      "Epoch 21/25\n",
      "2153/2153 [==============================] - 126s 58ms/step - loss: 0.1678\n",
      "Epoch 22/25\n",
      "2153/2153 [==============================] - 136s 63ms/step - loss: 0.1677\n",
      "Epoch 23/25\n",
      "2153/2153 [==============================] - 144s 67ms/step - loss: 0.1669\n",
      "Epoch 24/25\n",
      "2153/2153 [==============================] - 123s 57ms/step - loss: 0.1611\n",
      "Epoch 25/25\n",
      "2153/2153 [==============================] - 143s 67ms/step - loss: 0.1574\n",
      "CNN, Word Embeddings (0.856418918918919, 0.8438254899039138)\n"
     ]
    }
   ],
   "source": [
    "def create_cnn(input_size):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.25)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(256, 3, activation=\"tanh\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"tanh\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(6, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn(input_size)\n",
    "accuracy = train_model(classifier, train_seq_x, trainLabels, valid_seq_x,25, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.7384\n",
      "Epoch 2/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.5436\n",
      "Epoch 3/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.4617\n",
      "Epoch 4/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.3996\n",
      "Epoch 5/50\n",
      "2153/2153 [==============================] - 203s 94ms/step - loss: 0.3551\n",
      "Epoch 6/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.3182\n",
      "Epoch 7/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.2878\n",
      "Epoch 8/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.2650\n",
      "Epoch 9/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.2491\n",
      "Epoch 10/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.2286\n",
      "Epoch 11/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.2127\n",
      "Epoch 12/50\n",
      "2153/2153 [==============================] - 204s 95ms/step - loss: 0.2046\n",
      "Epoch 13/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1924\n",
      "Epoch 14/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1800\n",
      "Epoch 15/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1766\n",
      "Epoch 16/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1633\n",
      "Epoch 17/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1620\n",
      "Epoch 18/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1547\n",
      "Epoch 19/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1502\n",
      "Epoch 20/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1379\n",
      "Epoch 21/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1395\n",
      "Epoch 22/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1353\n",
      "Epoch 23/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1293\n",
      "Epoch 24/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1261\n",
      "Epoch 25/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1229\n",
      "Epoch 26/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1225\n",
      "Epoch 27/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1187\n",
      "Epoch 28/50\n",
      "2153/2153 [==============================] - 205s 95ms/step - loss: 0.1155\n",
      "Epoch 29/50\n",
      "2153/2153 [==============================] - 1376s 639ms/step - loss: 0.1135\n",
      "Epoch 30/50\n",
      "2153/2153 [==============================] - 206s 95ms/step - loss: 0.1103\n",
      "Epoch 31/50\n",
      "2153/2153 [==============================] - 206s 96ms/step - loss: 0.1080\n",
      "Epoch 32/50\n",
      "2153/2153 [==============================] - 212s 98ms/step - loss: 0.1059\n",
      "Epoch 33/50\n",
      "2153/2153 [==============================] - 279s 130ms/step - loss: 0.1044\n",
      "Epoch 34/50\n",
      "2153/2153 [==============================] - 262s 122ms/step - loss: 0.1021\n",
      "Epoch 35/50\n",
      "2153/2153 [==============================] - 261s 121ms/step - loss: 0.1007\n",
      "Epoch 36/50\n",
      "2153/2153 [==============================] - 260s 121ms/step - loss: 0.0999\n",
      "Epoch 37/50\n",
      "2153/2153 [==============================] - 261s 121ms/step - loss: 0.0972\n",
      "Epoch 38/50\n",
      "2153/2153 [==============================] - 261s 121ms/step - loss: 0.0955\n",
      "Epoch 39/50\n",
      "2153/2153 [==============================] - 261s 121ms/step - loss: 0.0971\n",
      "Epoch 40/50\n",
      "2153/2153 [==============================] - 261s 121ms/step - loss: 0.0907\n",
      "Epoch 41/50\n",
      "2153/2153 [==============================] - 261s 121ms/step - loss: 0.0908\n",
      "Epoch 42/50\n",
      "2153/2153 [==============================] - 279s 130ms/step - loss: 0.0915\n",
      "Epoch 43/50\n",
      "2153/2153 [==============================] - 298s 138ms/step - loss: 0.0919\n",
      "Epoch 44/50\n",
      "2153/2153 [==============================] - 326s 152ms/step - loss: 0.0861\n",
      "Epoch 45/50\n",
      "2153/2153 [==============================] - 264s 123ms/step - loss: 0.0868\n",
      "Epoch 46/50\n",
      "2153/2153 [==============================] - 264s 123ms/step - loss: 0.0869\n",
      "Epoch 47/50\n",
      "2153/2153 [==============================] - 263s 122ms/step - loss: 0.0863\n",
      "Epoch 48/50\n",
      "2153/2153 [==============================] - 287s 133ms/step - loss: 0.0857\n",
      "Epoch 49/50\n",
      "2153/2153 [==============================] - 265s 123ms/step - loss: 0.0841\n",
      "Epoch 50/50\n",
      "2153/2153 [==============================] - 264s 122ms/step - loss: 0.0835\n",
      "CNN, Word Embeddings (0.8366554054054054, 0.8183637349314613)\n"
     ]
    }
   ],
   "source": [
    "def create_cnn(input_size):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(300, 5, activation=\"relu\")(embedding_layer)\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.MaxPool1D()(conv_layer)\n",
    "     # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(256, 3, activation=\"relu\")(pooling_layer)\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "    \n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(256, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(6, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn(input_size)\n",
    "accuracy = train_model(classifier, train_seq_x, trainLabels, valid_seq_x,50, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2153/2153 [==============================] - 178s 83ms/step - loss: 0.7408\n",
      "Epoch 2/25\n",
      "2153/2153 [==============================] - 168s 78ms/step - loss: 0.5681\n",
      "Epoch 3/25\n",
      "2153/2153 [==============================] - 166s 77ms/step - loss: 0.5080\n",
      "Epoch 4/25\n",
      "2153/2153 [==============================] - 165s 77ms/step - loss: 0.4608\n",
      "Epoch 5/25\n",
      "2153/2153 [==============================] - 166s 77ms/step - loss: 0.4267\n",
      "Epoch 6/25\n",
      "2153/2153 [==============================] - 166s 77ms/step - loss: 0.3993\n",
      "Epoch 7/25\n",
      "2153/2153 [==============================] - 166s 77ms/step - loss: 0.3801\n",
      "Epoch 8/25\n",
      "2153/2153 [==============================] - 166s 77ms/step - loss: 0.3594\n",
      "Epoch 9/25\n",
      "2153/2153 [==============================] - 166s 77ms/step - loss: 0.3429\n",
      "Epoch 10/25\n",
      "2153/2153 [==============================] - 167s 77ms/step - loss: 0.3301\n",
      "Epoch 11/25\n",
      "2153/2153 [==============================] - 167s 78ms/step - loss: 0.3155\n",
      "Epoch 12/25\n",
      "2153/2153 [==============================] - 167s 77ms/step - loss: 0.3013\n",
      "Epoch 13/25\n",
      "2153/2153 [==============================] - 163s 76ms/step - loss: 0.2911\n",
      "Epoch 14/25\n",
      "2153/2153 [==============================] - 159s 74ms/step - loss: 0.2865\n",
      "Epoch 15/25\n",
      "2153/2153 [==============================] - 159s 74ms/step - loss: 0.2798\n",
      "Epoch 16/25\n",
      "2153/2153 [==============================] - 200s 93ms/step - loss: 0.2692\n",
      "Epoch 17/25\n",
      "2153/2153 [==============================] - 188s 87ms/step - loss: 0.2632\n",
      "Epoch 18/25\n",
      "2153/2153 [==============================] - 166s 77ms/step - loss: 0.2577\n",
      "Epoch 19/25\n",
      "2153/2153 [==============================] - 168s 78ms/step - loss: 0.2521\n",
      "Epoch 20/25\n",
      "2153/2153 [==============================] - 178s 83ms/step - loss: 0.2445\n",
      "Epoch 21/25\n",
      "2153/2153 [==============================] - 159s 74ms/step - loss: 0.2441\n",
      "Epoch 22/25\n",
      "2153/2153 [==============================] - 157s 73ms/step - loss: 0.2379\n",
      "Epoch 23/25\n",
      "2153/2153 [==============================] - 158s 73ms/step - loss: 0.2348\n",
      "Epoch 24/25\n",
      "2153/2153 [==============================] - 158s 73ms/step - loss: 0.2280\n",
      "Epoch 25/25\n",
      "2153/2153 [==============================] - 158s 73ms/step - loss: 0.2229\n",
      "CNN, Word Embeddings (0.8373310810810811, 0.8281086644526977)\n"
     ]
    }
   ],
   "source": [
    "def create_cnn(input_size):\n",
    "    # channel1\n",
    "    input_layer1 = layers.Input((input_size, ))\n",
    "    embedding_layer1 = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer1)\n",
    "    conv_layer1 = layers.Convolution1D(filters=256, kernel_size=3, activation=\"tanh\")(embedding_layer1)\n",
    "    drop_layer1 = layers.Dropout(0.25)(conv_layer1)\n",
    "    pooling_layer1 = layers.MaxPooling1D()(drop_layer1)\n",
    "    flaten_layer1 = layers.Flatten()(pooling_layer1)\n",
    "    \n",
    "    # channel2\n",
    "    input_layer2 = layers.Input((input_size, ))\n",
    "    embedding_layer2 = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer2)\n",
    "    conv_layer2 = layers.Convolution1D(filters=256, kernel_size=5, activation=\"tanh\")(embedding_layer2)\n",
    "    drop_layer2 = layers.Dropout(0.25)(conv_layer2)\n",
    "    pooling_layer2 = layers.MaxPooling1D()(drop_layer2)\n",
    "    flaten_layer2 = layers.Flatten()(pooling_layer2)\n",
    "    \n",
    "    # channel3\n",
    "    input_layer3 = layers.Input((input_size, ))\n",
    "    embedding_layer3 = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer3)\n",
    "    conv_layer3 = layers.Convolution1D(filters=256, kernel_size=7, activation=\"relu\")(embedding_layer3)\n",
    "    drop_layer3 = layers.Dropout(0.25)(conv_layer3)\n",
    "    pooling_layer3 = layers.MaxPooling1D()(drop_layer3)\n",
    "    flaten_layer3 = layers.Flatten()(pooling_layer3)\n",
    "    \n",
    "    # merge\n",
    "    merged_layer = layers.concatenate([flaten_layer1, flaten_layer2, flaten_layer3])\n",
    "    # interpretation\n",
    "    dense_layer = layers.Dense(128, activation='relu')(merged_layer)\n",
    "    outputs = layers.Dense(6, activation='softmax')(dense_layer)\n",
    "    model = models.Model(inputs=[input_layer1, input_layer2, input_layer3], outputs=outputs)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn(input_size)\n",
    "accuracy = train_model(classifier, [train_seq_x,train_seq_x,train_seq_x] , trainLabels, [valid_seq_x, valid_seq_x, valid_seq_x], 25, is_neural_net=True)\n",
    "print(\"CNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2153/2153 [==============================] - 78s 36ms/step - loss: 0.7048\n",
      "Epoch 2/25\n",
      "2153/2153 [==============================] - 77s 36ms/step - loss: 0.4910\n",
      "Epoch 3/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.4149\n",
      "Epoch 4/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.3599\n",
      "Epoch 5/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.3263\n",
      "Epoch 6/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.2996\n",
      "Epoch 7/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.2695\n",
      "Epoch 8/25\n",
      "2153/2153 [==============================] - 75s 35ms/step - loss: 0.2509\n",
      "Epoch 9/25\n",
      "2153/2153 [==============================] - 76s 36ms/step - loss: 0.2360\n",
      "Epoch 10/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.2183\n",
      "Epoch 11/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.2079\n",
      "Epoch 12/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.1988\n",
      "Epoch 13/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.1911\n",
      "Epoch 14/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.1838\n",
      "Epoch 15/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.1747\n",
      "Epoch 16/25\n",
      "2153/2153 [==============================] - 76s 35ms/step - loss: 0.1717\n",
      "Epoch 17/25\n",
      "2153/2153 [==============================] - 78s 36ms/step - loss: 0.1633\n",
      "Epoch 18/25\n",
      "2153/2153 [==============================] - 81s 37ms/step - loss: 0.1604\n",
      "Epoch 19/25\n",
      "2153/2153 [==============================] - 81s 37ms/step - loss: 0.1554\n",
      "Epoch 20/25\n",
      "2153/2153 [==============================] - 81s 38ms/step - loss: 0.1474\n",
      "Epoch 21/25\n",
      "2153/2153 [==============================] - 81s 38ms/step - loss: 0.1440\n",
      "Epoch 22/25\n",
      "2153/2153 [==============================] - 81s 38ms/step - loss: 0.1427\n",
      "Epoch 23/25\n",
      "2153/2153 [==============================] - 81s 38ms/step - loss: 0.1391\n",
      "Epoch 24/25\n",
      "2153/2153 [==============================] - 81s 38ms/step - loss: 0.1365\n",
      "Epoch 25/25\n",
      "2153/2153 [==============================] - 81s 38ms/step - loss: 0.1334\n",
      "RCNN, Word Embeddings (0.8530405405405406, 0.8363414283770992)\n"
     ]
    }
   ],
   "source": [
    "def create_rcnn(input_size=100):\n",
    "\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((input_size, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.2)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.LSTM(256, return_sequences=True, dropout=0.25,recurrent_dropout=0.25))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(256, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(128, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(6, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "accuracy = train_model(classifier, train_seq_x, trainLabels, valid_seq_x, 25, is_neural_net=True)\n",
    "print(\"RCNN, Word Embeddings\",  accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
